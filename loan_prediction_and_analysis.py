# -*- coding: utf-8 -*-
"""loan_prediction_and_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mHKRymRiq7dR_vZk5IaE8tjiRV9Np692
"""



"""# Loan Prediction Analysis
* Loan privileges can also be said to be credit worthiness, which is how a lender evaluates
whether or not you will default on your debt obligations, as well as whether or not you are
eligible for a credit or additional credit. The amount of money you can borrow is determined by a
number of factors, including your repayment history and credit score. When determining the
possibility of default, some lending organizations take into action your available asset as well as
the quantity of liabilities you have.
* In finance, Loan is lending help for a induvidual (or) organisation by the other induvidual (or) organization. But it has it's own merits and demerits. One such is the trust that money lender should have on the load borrower. This trust needs to meet some requirements like collateral and their monthly earnings. This notebook is just a simple example for such validation using Machine learning
### Statement of the Problem 
* Decisions on traditional bank loans are made using credit scores along with data from
application forms and credit reference agencies, which can be cumbersome, error prone and
often biased process. Another problem is the guarantorâ€™s problem, where guarantors are not
aware of their status of standing in for a loan application.
Due to inadequate credit checks, weak intermediation, a lack of transparency, and the
inherent financial situation of typical internet browsers, online lending has higher risks than
traditional bank consumer loans. Therefore, credit risks prediction and management become
vitally important.
### AIM
* The aim of this research work is to propose a machine learning model that will help
check for irregularities during the application of online loans, irregularities such as the
unawareness of the guarantor as regards the loan until it is time for repayment. Also, its aim is to
use machine learning to determine who is and who is not able to secure loans from this app and
how the conditions or criteria needed to qualify for a loan are easily determined by machine
learning.
### Data Description 

### Importing Required Libraries
"""
import os
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
# import seaborn as sns
from sklearn.preprocessing import LabelEncoder,StandardScaler

"""### Loading Dataset"""
FILE_DIR = os.path.dirname(os.path.abspath(__file__))
# absolute path to this file's root directory
dir_of_interest = os.path.join(FILE_DIR, "data")
testpath = os.path.join(dir_of_interest, "test.csv")
trainpath = os.path.join(dir_of_interest, "train.csv")

train_df = pd.read_csv(trainpath) # Train data
test_df = pd.read_csv(testpath)


# Creating copy
train_copy = train_df.copy()
test_copy = test_df.copy()

#train_copy.head() # View dataset

#train_df['Credit_History'].value_counts()

"""### Exploring Data"""

#Viewing Data Description

# train_df.describe()

# train_df.isna().sum()

"""##### Report on missing data
* Base on the above visualization there are quite a lot of missing entry at various columns 
* Gender column has 13 missing value | Could be fix with Mode value
* Married column has 3 missing value | Could also be fix with mode value
* Dependent has about 15 missing values | Since there are chances that people without dependent might not have any we could fix this with 0
* Self Employed has 32 missing values | We can fix this with fill na forward option
* LoanAmount has 22 missing values | We can fix this with mean value
* Loan Amount Term has 14 missing values | We can fix this with mode value of the column
* credit history has 50 missing value | We can't possible drop the credit history due to its large correlation to our dependent variable
* Convert 3+ values in Dependent column to 3
"""

# Fixing missing values
# Gender
train_copy.Gender = train_copy.Gender.fillna(train_copy.Gender.mode()[0])
test_copy.Gender = test_copy.Gender.fillna(test_copy.Gender.mode()[0])

# Married
train_copy.Married = train_copy.Married.fillna(train_copy.Married.mode()[0])
test_copy.Married= test_copy.Married.fillna(test_copy.Married.mode()[0])

# Dependents
train_copy.Dependents = train_copy.Dependents.fillna('0')
test_copy.Dependents = test_copy.Dependents.fillna('0')
train_copy.Dependents = train_copy.Dependents.replace('3+','3')
test_copy.Dependents = test_copy.Dependents.replace('3+','3')
pd.to_numeric(train_copy['Dependents'])
# Self_Employed
train_copy.Self_Employed = train_copy.Self_Employed.fillna(method='ffill')
test_copy.Self_Employed = test_copy.Self_Employed.fillna(method='ffill')

# Loan Amount
train_copy.LoanAmount = train_copy.LoanAmount.fillna(train_copy.LoanAmount.mean())
test_copy.LoanAmount = test_copy.LoanAmount.fillna(test_copy.LoanAmount.mean())

# Loan Amount Term
train_copy.Loan_Amount_Term = train_copy.Loan_Amount_Term.fillna(train_copy.Loan_Amount_Term.mean())
test_copy.Loan_Amount_Term = test_copy.Loan_Amount_Term.fillna(test_copy.Loan_Amount_Term.mean())

# ffil credit history
train_copy.Credit_History = train_copy.Credit_History.fillna('0')
test_copy.Credit_History = test_copy.Credit_History.fillna('0')
pd.to_numeric(train_copy['Credit_History'])

# Check For Changes in Missing Values in Dataset

#train_copy.isna().sum()

"""#### Tidy Data
* Some columns are not in correct datatype e.g Loan amount and Dependent

"""

#Checking DataTypes

#train_copy.info()

train_copy.Dependents=train_copy.Dependents.astype(int)
test_copy.Dependents=test_copy.Dependents.astype(int)

train_copy.LoanAmount=train_copy.LoanAmount.astype(int)
test_copy.LoanAmount=test_copy.LoanAmount.astype(int)

train_copy.Credit_History=train_copy.Credit_History.astype(int)
test_copy.Credit_History=test_copy.Credit_History.astype(int)

#Check for Changes
#train_copy.info()

"""#### Exploratory Analysis"""

# The Histogram distribution of the Loan Amount
#sns.distplot(train_copy.LoanAmount)
#plt.show()

#sns.boxplot(x='Loan_Status', y='LoanAmount', data=train_copy)
#plt.show()

"""There is no much bias on the loans that were approved
   Or regected in respect to the amount of loan requested for"""

#sns.countplot(x='Loan_Status', hue='Credit_History', data=train_copy)
#plt.show()

""" Majority of the Loans that were approved has a credit history
    While majority of the loans that were regected does not have a credit history
    Althoug some of the loans with a credit history were rejected while 
    some of the loan without a credit history were accepted 
    This plot shows the correlation between our target column and the credit history
    """

#plt.scatter(train_copy['ApplicantIncome'], train_copy['LoanAmount'])
#plt.xlabel('Applicant Income')
#plt.ylabel('Loan Amount')
#plt.show()

"""##### Exploratory Report
* The Loan Amount is left skewed which is as a reason of outliers we can fix this by performing Log Transformation on the data
* There is no much bias on the loans that were approved or regected in respect to the amount of loan requested for
* Majority of the Loans that were approved has a credit history while majority of the loans that were regected does not have a credit history although some of the loans with a credit history were rejected while some of the loan without a credit history were accepted. This plot shows the correlation between our target column and the credit history
* Also there are few outliers on the applicant income column

### Encoding Data
* The Property Area and The Loan Status Columns will be encoded using the Label Encoding 
* Other Column will be encoded using one hot encoding techniques withe the dummies function in pandas
"""

# Performing Label Encoding
propertydict = []
label_encoding_categorical_columns = ['Property_Area', 'Loan_Status']
encoder = LabelEncoder()

for column in label_encoding_categorical_columns:
  encoded = encoder.fit_transform(train_copy[column])
  train_copy.drop(column, axis=1, inplace=True)
  train_copy[column] = encoded
  feature_dict = dict(zip(range(len(encoder.classes_)), encoder.classes_))
  propertydict.append(feature_dict)

  if column == 'Loan_Status':
    break

  encoded_test = encoder.fit_transform(test_copy[column])
  test_copy.drop(column, axis=1, inplace=True)
  test_copy[column] = encoded_test

# Performing One Hot Encoding
onehot_encoding_categorical_columns = ['Gender', 'Married', 'Education','Self_Employed']

train_copy = pd.get_dummies(train_copy, columns = onehot_encoding_categorical_columns,drop_first=True)
test_copy = pd.get_dummies(test_copy, columns = onehot_encoding_categorical_columns,drop_first=True)

"""#### Drop Unnecessary Column
* The Id column is a column that has no null value and all unique column therefor it will has no meaning to our dataset hence its advisable we drop it.
"""

train_copy.drop('Loan_ID',axis=1,inplace=True)
test_copy.drop('Loan_ID',axis=1,inplace=True)

#train_copy.head()

"""#### Feature engineering

* Create a column for Totalhousehold_income which is a result of applicantincome + coapplicantincome

"""

train_copy['Totalhouseincome'] = train_copy['ApplicantIncome'] + train_copy['CoapplicantIncome']
test_copy['Totalhouseincome'] = test_copy['ApplicantIncome'] + test_copy['CoapplicantIncome']

#train_copy.head()

"""#### Feature Scaling
* We perform feature scaling using standard scaler
"""

numerical_columns = ['Dependents','ApplicantIncome','CoapplicantIncome','LoanAmount','Totalhouseincome']
numerical_df_train = train_copy[numerical_columns]
numerical_df_test = test_copy[numerical_columns]
scaler = StandardScaler()

scaled_train = pd.DataFrame(scaler.fit_transform(numerical_df_train),columns=numerical_columns)
scaled_test = pd.DataFrame(scaler.transform(numerical_df_test),columns=numerical_columns)

final_train = pd.concat([train_copy.drop(numerical_columns, axis=1), scaled_train], axis=1)
final_validation = pd.concat([test_copy.drop(numerical_columns, axis=1), scaled_test], axis=1)

# final_train.head()

from sklearn.model_selection import train_test_split
y= final_train['Loan_Status']
X = final_train.drop('Loan_Status', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestClassifier

classifier = RandomForestClassifier()

#Train Model
classifier.fit(X_train,y_train)

# #Prediction 
# y_pred = classifier.predict(X_test)

# # Evaluate Model
# from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score

# accuracy = accuracy_score(y_test, y_pred)
# precision = precision_score(y_test, y_pred, average='weighted')
# recall = recall_score(y_test, y_pred, average='weighted')

# # Create confusion matrix
# conf_matrix = confusion_matrix(y_test, y_pred)

# print(f"Accuracy: {accuracy:.2f}")
# print(f"Precision: {precision:.2f}")
# print(f"Recall: {recall:.2f}")
# print(f"Confusion matrix:\n{conf_matrix}")



